



Not all processing will be successful. I want to alert the user when a job fails and give them options for how to proceed
use st.radio
make use of keys and st.session_state and flags
The options are to:
- retry failed jobs (call resume_job(try_failed_jobs=True))
- skip failed jobs  (call resume_job(try_failed_jobs=False))
- abort processing  (st.session_state.jobs_dict["to_process] = [])
- wait
I have brought in part of an app from another repo--Transcriber.py--for ideas of how to handle this
The trick is not to generate errors by creating multiple verions of the same st.radio 


##########################################

It's time to get ready for a commit
Before I do, I want to do these things:
1) get rid of unnecessary files. look for old files that are no longer need or that were there just to debug
2) the app seems to work, so it's time to add error handling so there are not runtime errors that crash the whole things
3) update the README
4) take creating the gitignore out of the setup
5) look for data files and their directories that should be part of the gitignore

If you questions about what files should be eliminated, ask




###################################


Good. Now modify the app so models that passed the image test can be selected.
These items should be addressed:
1) If an inference profile is available, use that when calling the model
2) For this iteration, use a text prompt and get the transcription as json--if the model supports that--otherwise get the transcription as text and convert it to a json.
3) Improve error messaging with a focus on letting  the user know why a transcript was not provided



####################################
re-write setup.py with the following goals in mind:
1) retain current overall functionality and create a file vision_model_info.json, which will later be used in the app for the user to select a model to process images
2) vision_model_info.json will contain information like the current one, but will contain ALL the models I have access to that support image inputs, regardless of image_test_success and regardless of whether they they support on-demand processing or an inference profile. Those things will remain noted though. 
3) if all goes right, vision_model_info.json should contain at a minimum 3-5-sonnet-v-1, 3-5-sonnet-v-2, various nova models and various llama models.
4) save changes to the app for later, but make all changes to other files necessary to create that json.
5) consolidate model testing to one file
6) consolidate model information retrieval, e.g., getting info from Bedrock, to one 
7) you have permission to edit, create and test any files



#####################################

To call the LLaMA model using boto3 for image transcription, you will need to use the `invoke_endpoint` method of the `SageMakerRuntime` client. Below is a basic example of how you can structure your API call. This example assumes you have the necessary credentials set up for your AWS account and that you have the ARN of the LLaMA endpoint you want to invoke.

First, ensure you have the necessary imports and have set up your AWS credentials:

```python
import boto3
import base64
from botocore.exceptions import ClientError

# Initialize the SageMaker Runtime client
sagemaker_runtime = boto3.client('sagemaker-runtime')
```

Then, you need to prepare your image and prompt. You've mentioned that you already have your prompt text and a base64-encoded image. Here's how you might structure the data for the API call:

```python
# Your base64 encoded image
image_b64 = "your_base64_image_string_here"

# Your prompt text
prompt_text = "Transcribe the text in this image:"

# Assuming the endpoint expects a JSON payload with 'prompt' and 'image'
payload = {
    'prompt': prompt_text,
    'image': image_b64
}

# Convert the payload to JSON
import json
payload_json = json.dumps(payload)
```

Next, you'll need the ARN of your LLaMA endpoint. Replace `'YOUR_ENDPOINT_NAME'` with the actual name of your endpoint.

```python
# The name of your SageMaker endpoint
endpoint_name = 'YOUR_ENDPOINT_NAME'

# Invoke the endpoint
try:
    response = sagemaker_runtime.invoke_endpoint(
        EndpointName=endpoint_name,
        ContentType='application/json',
        Body=payload_json
    )
    # Process the response
    result = json.loads(response['Body'].read())
    print(result)
except ClientError as e:
    print(e.response['Error']['Message'])
```

Please note the following:
- Ensure your AWS credentials are properly configured.
- Replace `'YOUR_ENDPOINT_NAME'` with the actual name of your SageMaker endpoint.
- The `ContentType` parameter is set to `'application/json'` assuming your endpoint expects JSON input. Adjust this if your endpoint expects a different content type.
- Error handling is basic in this example. Depending on your application, you may want to implement more robust error handling.

This example should give you a starting point for making the API call. However, the specifics can vary depending on how your LLaMA model endpoint is configured and what it expects as input. Always refer to the documentation for your specific endpoint for detailed instructions.