




####################################
re-write setup.py with the following goals in mind:
1) retain current overall functionality and create a file vision_model_info.json, which will later be used in the app for the user to select a model to process images
2) vision_model_info.json will contain information like the current one, but will contain ALL the models I have access to that support image inputs, regardless of image_test_success and regardless of whether they they support on-demand processing or an inference profile. Those things will remain noted though. 
3) if all goes right, vision_model_info.json should contain at a minimum 3-5-sonnet-v-1, 3-5-sonnet-v-2, various nova models and various llama models.
4) save changes to the app for later, but make all changes to other files necessary to create that json.
5) consolidate model testing to one file
6) consolidate model information retrieval, e.g., getting info from Bedrock, to one file



#####################################

To call the LLaMA model using boto3 for image transcription, you will need to use the `invoke_endpoint` method of the `SageMakerRuntime` client. Below is a basic example of how you can structure your API call. This example assumes you have the necessary credentials set up for your AWS account and that you have the ARN of the LLaMA endpoint you want to invoke.

First, ensure you have the necessary imports and have set up your AWS credentials:

```python
import boto3
import base64
from botocore.exceptions import ClientError

# Initialize the SageMaker Runtime client
sagemaker_runtime = boto3.client('sagemaker-runtime')
```

Then, you need to prepare your image and prompt. You've mentioned that you already have your prompt text and a base64-encoded image. Here's how you might structure the data for the API call:

```python
# Your base64 encoded image
image_b64 = "your_base64_image_string_here"

# Your prompt text
prompt_text = "Transcribe the text in this image:"

# Assuming the endpoint expects a JSON payload with 'prompt' and 'image'
payload = {
    'prompt': prompt_text,
    'image': image_b64
}

# Convert the payload to JSON
import json
payload_json = json.dumps(payload)
```

Next, you'll need the ARN of your LLaMA endpoint. Replace `'YOUR_ENDPOINT_NAME'` with the actual name of your endpoint.

```python
# The name of your SageMaker endpoint
endpoint_name = 'YOUR_ENDPOINT_NAME'

# Invoke the endpoint
try:
    response = sagemaker_runtime.invoke_endpoint(
        EndpointName=endpoint_name,
        ContentType='application/json',
        Body=payload_json
    )
    # Process the response
    result = json.loads(response['Body'].read())
    print(result)
except ClientError as e:
    print(e.response['Error']['Message'])
```

Please note the following:
- Ensure your AWS credentials are properly configured.
- Replace `'YOUR_ENDPOINT_NAME'` with the actual name of your SageMaker endpoint.
- The `ContentType` parameter is set to `'application/json'` assuming your endpoint expects JSON input. Adjust this if your endpoint expects a different content type.
- Error handling is basic in this example. Depending on your application, you may want to implement more robust error handling.

This example should give you a starting point for making the API call. However, the specifics can vary depending on how your LLaMA model endpoint is configured and what it expects as input. Always refer to the documentation for your specific endpoint for detailed instructions.